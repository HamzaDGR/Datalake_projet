# Projet Data Lake OpenSky

## Introduction
Ce projet vise √† concevoir et impl√©menter un Data Lake en utilisant diverses technologies modernes de traitement et de stockage des donn√©es. L'objectif est de collecter, ing√©rer et stocker des donn√©es a√©ronautiques issues d'OpenSky Network, tout en permettant leur analyse via une pipeline de donn√©es automatis√©e.

Le projet repose sur les technologies suivantes :
- **Docker & Docker Compose** pour la gestion des services
- **Airflow** pour l'orchestration des pipelines de donn√©es
- **AWS S3 (via LocalStack)** pour le stockage des donn√©es
- **MySQL** pour le stockage relationnel des donn√©es structur√©es
- **MongoDB** pour le stockage NoSQL des donn√©es semi-structur√©es
- **FastAPI** pour exposer les donn√©es via une API REST

## Open Sky

L'API OpenSky REST est une interface de programmation d'applications qui permet d'acc√©der √† des donn√©es en temps r√©el sur les avions et leur position, collect√©es √† partir de milliers de capteurs et radars r√©partis dans le monde entier. Elle offre une m√©thode simple et directe pour obtenir des informations li√©es au trafic a√©rien, telles que les positions, les trajets, les horaires, etc. C'est √† partir de cette API que nous collecterons les donn√©es.

---

## Architecture Generale 

![architecture drawio](https://github.com/user-attachments/assets/16df8122-bb68-41ed-abbe-1ab07e5ae92b)



## Installation

### 1. Installer les d√©pendances
Il est recommand√© d'utiliser un environnement virtuel Python :

```sh
venv\Scripts\Activate  
pip install -r requirements.txt
```

### 2. Lancer les services Docker
D√©marrez Docker, puis ex√©cutez la commande suivante pour lancer les conteneurs :

```sh
docker-compose up --build -d
```

### 3. Installer `pymongo` dans le conteneur Airflow

```sh
docker exec -it data-lake-opensky-airflow-worker-1 bash
pip install pymongo
exit
```

---

## Utilisation

### 1. Acc√©der √† l'interface Airflow
Ouvrez votre navigateur et connectez-vous avec les identifiants `airflow / airflow` :

```sh
http://localhost:8080/home
```

### 2. Ex√©cuter le pipeline de donn√©es
Dans l'interface Airflow, localisez le DAG nomm√© **data_ingestion** et cliquez sur **Trigger DAG** sous la colonne **Actions**.

Le pipeline va automatiquement :
- Collecter les donn√©es de l'API OpenSky Network
- Stocker les donn√©es dans un bucket S3
- Charger les donn√©es dans MySQL et MongoDB

---

## Validation des donn√©es

### 1. V√©rifier les fichiers dans le bucket S3
Utilisez la commande suivante pour lister les fichiers stock√©s :

```sh
aws --endpoint-url=http://localhost:4566 s3 ls s3://open-sky-datalake-bucket/ --recursive
```

#### üì∏ Capture d‚Äô√©cran du stockage S3 
![Capture d‚Äô√©cran 2025-03-12 001409](https://github.com/user-attachments/assets/b9ab4a73-306a-47a1-bee2-d50463e2efa0)


### 2. V√©rifier les donn√©es dans MySQL
Acc√©dez au conteneur MySQL et effectuez une requ√™te de v√©rification :

```sh
docker exec -it mysql mysql -u root -p 
use staging;
select * from flights limit 5;
```

#### üì∏ Capture d‚Äô√©cran de la base de donn√©es MySQL 

![Capture d‚Äô√©cran 2025-03-12 001943](https://github.com/user-attachments/assets/f9209abf-f9d8-4b69-a3ee-878070f6fba2)

### 3. V√©rifier les donn√©es dans MongoDB Compass
Ouvrez MongoDB Compass et connectez-vous √† votre base de donn√©es pour explorer les documents.

#### üì∏ Capture d‚Äô√©cran de MongoDB Compass 

![Capture d‚Äô√©cran 2025-03-12 001809](https://github.com/user-attachments/assets/0180961a-d87f-4b38-aa3d-a6e9dba8bf17)

---
## Requ√™ter les donn√©es via l'API

### Une API REST permet d'acc√©der aux donn√©es stock√©es dans le Data Lake.

### 1. R√©cup√©rer les donn√©es brutes depuis le bucket S3

Utilisez l'URL suivante pour acc√©der aux fichiers raw :

http://127.0.0.1:8000/raw/all_content?bucket=open-sky-datalake-bucket&prefix=raw_data/

![image](https://github.com/user-attachments/assets/2e16357e-c53d-4a18-bdc9-0f07726d0fb2)


### 2. R√©cup√©rer les donn√©es cur√©es depuis MongoDB

Utilisez l'URL suivante pour interroger la collection des vols :

http://127.0.0.1:8000/curated?collection=flights

![Capture d‚Äô√©cran 2025-03-12 004417](https://github.com/user-attachments/assets/2fc3b038-4578-40f9-a6ac-1c838bbd8f67)

Les autres API sont en cours de developpement 

### Objectif Final : Dashboard Streamlit

L'objectif final de ce projet est de d√©velopper un dashboard interactif avec Streamlit, permettant de visualiser et d'analyser les donn√©es collect√©es. Ce dashboard proposera plusieurs graphiques et indicateurs cl√©s pour mieux comprendre les tendances et les statistiques des vols a√©riens.



## Auteurs

### üë• Contributeurs
- **Tom-Hugues ALLARD** - [tom-hugues.allard@efrei.net](mailto:tom-hugues.allard@efrei.net)  
- **Hamza DOUGAREM** - [hamza.dougarem@efrei.net](mailto:hamza.dougarem@efrei.net)

Merci pour votre int√©r√™t et votre retour ! üöÄ




